{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "# import onnxruntime\n",
    "import numpy as np\n",
    "import time\n",
    "from model import transfer_resNet, ResNet54, ResNet22, ResNet38, resnet50, MMNet\n",
    "from audiodataset import CoswaraDataset, ConcatDataset, CoswaraDataset2\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from train import train, validation, mm_train\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer Name: pytorch\n",
      "Producer Version: 1.7\n",
      "Opset version: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "onnx_model = onnx.load('a_model.onnx')\n",
    "print(\"Producer Name:\", onnx_model.producer_name)\n",
    "print(\"Producer Version:\", onnx_model.producer_version)\n",
    "print(\"Opset\", onnx_model.opset_import[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "a_model = transfer_resNet(2)\n",
    "# image\n",
    "i_model = resnet50(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('/model/save1/f_a_model_0.923_0.923')\n",
    "a_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test examples =  472 [439  33]\n"
     ]
    }
   ],
   "source": [
    "# audio dataset\n",
    "data = np.load('audios_full.npz', allow_pickle=True)\n",
    "aX, ay = data['x'], data['y']\n",
    "data = np.load('images_tr_full.npz', allow_pickle=True)\n",
    "iX_tr, y_tr = data['x'], data['y']\n",
    "data = np.load('images_test.npz', allow_pickle=True)\n",
    "iX_te, y_te = data['x'], data['y']\n",
    "\n",
    "batch_size = 96\n",
    "test_params = {'batch_size': batch_size,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 15}\n",
    "\n",
    "seed = 20\n",
    "seed_everything(seed)\n",
    "aX_tr, aX_te, ay_tr, ay_te = train_test_split(aX, ay, test_size=0.2, shuffle=True, stratify=ay, random_state=seed)\n",
    "cos_test_dataset = CoswaraDataset2(aX_te, ay_te, mode='test')\n",
    "o_test_loader = DataLoader(cos_test_dataset, **test_params)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      " SUMMARY EPOCH: 0\tSample:  472/  472\tLoss:0.2743\tAccuracy:0.92\n",
      "\n",
      "Confusion Matrix\n",
      "[[423.  16.]\n",
      " [ 20.  13.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.923728813559322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_model.to(device)\n",
    "val_metrics, confusion_matrix = validation(device, batch_size, 2, a_model, o_test_loader, 0, None)\n",
    "val_metrics.avg('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "a_model = transfer_resNet(2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "a_model.to(device)\n",
    "optimal_batch_size = 64\n",
    "a_input = torch.randn(optimal_batch_size, 1, 80, 401).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmnet = MMNet()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mmnet.to(device)\n",
    "# state_dict = torch.load('model/mmnet1')\n",
    "# mmnet.load_state_dict(state_dict)\n",
    "optimal_batch_size = 16\n",
    "a_input = torch.randn(optimal_batch_size, 1, 80, 401).cuda() # input size - (batch_size, 1, time_steps, mel_bins)\n",
    "i_input = torch.randn(optimal_batch_size, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Throughput :  493.3935372844218\n",
      "0.12971390008926392\n"
     ]
    }
   ],
   "source": [
    "# pytorch Throughput - audio\n",
    "a_model.eval()\n",
    "repetitions=100\n",
    "total_time = 0\n",
    "pytorch_time = []\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        tic = time.time()\n",
    "        _ = a_model(a_input)\n",
    "        toc = time.time()\n",
    "        curr_time = toc - tic\n",
    "        total_time += curr_time\n",
    "        pytorch_time.append(curr_time)\n",
    "Throughput = (repetitions*optimal_batch_size)/total_time\n",
    "print('Final Throughput : ',Throughput)\n",
    "print(np.mean(pytorch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Throughput :  191.89291193458865\n",
      "0.08337983846664429\n"
     ]
    }
   ],
   "source": [
    "# pytorch Throughput - mmnet\n",
    "mmnet.eval()\n",
    "repetitions=100\n",
    "total_time = 0\n",
    "pytorch_time = []\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        tic = time.time()\n",
    "        _ = mmnet([a_input, i_input])\n",
    "        toc = time.time()\n",
    "        curr_time = toc - tic\n",
    "        total_time += curr_time\n",
    "        pytorch_time.append(curr_time)\n",
    "Throughput = (repetitions*optimal_batch_size)/total_time\n",
    "print('Final Throughput : ',Throughput)\n",
    "print(np.mean(pytorch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12996405363082886\n"
     ]
    }
   ],
   "source": [
    "# pytorch 시간 - audio\n",
    "a_model.eval()\n",
    "pytorch_time = []\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        tic = time.time()\n",
    "        outputs = a_model(a_input)\n",
    "        toc = time.time()\n",
    "        dur = toc - tic\n",
    "        pytorch_time.append(dur)\n",
    "print(np.mean(pytorch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01368621587753296\n"
     ]
    }
   ],
   "source": [
    "# pytorch 시간 - mmnet\n",
    "mmnet.eval()\n",
    "pytorch_time = []\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        tic = time.time()\n",
    "        outputs = mmnet([a_input, i_input])\n",
    "        toc = time.time()\n",
    "        dur = toc - tic\n",
    "        pytorch_time.append(dur)\n",
    "print(np.mean(pytorch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model.to('cuda')\n",
    "a_model.eval()\n",
    "a_input = torch.randn(1, 1, 80, 401).cuda() # input size - (batch_size, 1, mel_bins, time_steps)\n",
    "dummy_input = a_input\n",
    "\n",
    "input_names = [ \"input\"]\n",
    "output_names = [ \"a_output\", \"a_fe\"]\n",
    "\n",
    "torch.onnx.export(a_model, dummy_input, \"a_model.onnx\",\n",
    "                input_names=input_names, \n",
    "                output_names=output_names,\n",
    "#                 opset_version=11,\n",
    "#                 verbose=True,\n",
    "                export_params=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmnet = MMNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx - mmnet\n",
    "mmnet.to('cuda')\n",
    "mmnet.eval()\n",
    "optimal_batch_size = 64\n",
    "a_input = torch.randn(optimal_batch_size, 1, 80, 401).cuda() # input size - (batch_size, 1, time_steps, mel_bins)\n",
    "i_input = torch.randn(optimal_batch_size, 3, 224, 224).cuda()\n",
    "dummy_input = [a_input, i_input]\n",
    "\n",
    "input_names = [ \"a_input\", \"i_input\" ]\n",
    "output_names = [ \"output\" ]\n",
    "\n",
    "torch.onnx.export(mmnet, dummy_input, \"mmnet_64.onnx\",\n",
    "                input_names=input_names, \n",
    "                output_names=output_names,\n",
    "                export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX file from path mmnet_32.onnx...\n",
      "Beginning ONNX file parsing\n",
      "Completed parsing of ONNX file\n",
      "Building an engine; this may take a while...\n",
      "Completed creating engine\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "onnx_file_name = 'mmnet_32.onnx'\n",
    "tensorrt_file_name = 'mmnet_32.trt'\n",
    "fp16_mode = True\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "builder.max_batch_size = 64\n",
    "builder.max_workspace_size = (1 << 30)\n",
    "builder.fp16_mode = fp16_mode\n",
    "\n",
    "# Parse model file\n",
    "print('Loading ONNX file from path {}...'.format(onnx_file_name))\n",
    "with open(onnx_file_name, 'rb') as model:\n",
    "    print('Beginning ONNX file parsing')\n",
    "    if not parser.parse(model.read()):\n",
    "        print('ERROR: Failed to parse the ONNX file.')\n",
    "        for error in range(parser.num_errors):\n",
    "            print (parser.get_error(error))\n",
    "print('Completed parsing of ONNX file')\n",
    "\n",
    "print('Building an engine; this may take a while...')\n",
    "engine = builder.build_cuda_engine(network)\n",
    "print('Completed creating engine')\n",
    "buf = engine.serialize()\n",
    "with open(tensorrt_file_name, 'wb') as f:\n",
    "    f.write(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load TensorRT File & TensorRT Inference #################\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "device = cuda.Device(0)\n",
    "context = device.make_context()\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    " \n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * 64 #engine.max_batch_size\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "#     ctx.pop()\n",
    "#     del ctx\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def trt_init():\n",
    "    tensorrt_file_name = 'a_model.trt'\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "    with open(tensorrt_file_name, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine\n",
    "\n",
    "# This function is generalized for multiple inputs/outputs.\n",
    "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # [cuda.memcpy_htod(inp.device, inp.host) for inp in inputs]\n",
    "    # Run inference.\n",
    "    tic = time.time()\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    toc = time.time()\n",
    "    dur = toc - tic\n",
    "    # context.execute(batch_size=batch_size, bindings=bindings)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # [cuda.memcpy_dtoh(out.host, out.device) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "\n",
    "    return [out.host for out in outputs], dur\n",
    "\n",
    "\n",
    "engine = trt_init()\n",
    "# inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "inputs = []\n",
    "outputs = []\n",
    "bindings = []\n",
    "stream = cuda.Stream()\n",
    "for binding in engine:\n",
    "    size = trt.volume(engine.get_binding_shape(binding)) * 1 #engine.max_batch_size\n",
    "    dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "    # Allocate host and device buffers\n",
    "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "    device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "    # Append the device buffer to device bindings.\n",
    "    bindings.append(int(device_mem))\n",
    "    # Append to the appropriate list.\n",
    "    if engine.binding_is_input(binding):\n",
    "        inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    else:\n",
    "        outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = np.load('mels_full.npz', allow_pickle=True)\n",
    "aX, ay = data['x'], data['y']\n",
    "aX_tr, aX_te, ay_tr, ay_te = train_test_split(aX, ay, test_size=0.2, shuffle=True, stratify=ay, random_state=20)\n",
    "ay_te = np.where(ay_te == 'healthy', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472, 1, 80, 401)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_input, image_input = torch.ones(1,1,80,401), torch.ones([1, 3, 224, 224])\n",
    "a_input = aX_te\n",
    " \n",
    "numpy_array_input = a_input\n",
    "hosts = [input.host for input in inputs]\n",
    "trt_types = [trt.int32, trt.int32]\n",
    " \n",
    "for numpy_array, host, trt_type in zip(numpy_array_input, hosts, trt_types):\n",
    "    numpy_array = np.asarray(numpy_array).astype(trt.nptype(trt_type)).ravel()\n",
    "    np.copyto(host, numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? -> dataload\n",
    "# 생성한 numpy array를 TensorRT가 읽을 수 있는 데이터로 변환 후 input의 입력으로 할당합니다.\n",
    "audio_input, image_input = torch.ones(1,1,80,401), torch.ones([1, 3, 224, 224])\n",
    "a_input, i_input = audio_input.numpy(), image_input.numpy()\n",
    " \n",
    "numpy_array_input = [a_input, i_input]\n",
    "hosts = [input.host for input in inputs]\n",
    "trt_types = [trt.int32, trt.int32]\n",
    " \n",
    "for numpy_array, host, trt_type in zip(numpy_array_input, hosts, trt_types):\n",
    "    numpy_array = np.asarray(numpy_array).astype(trt.nptype(trt_type)).ravel()\n",
    "    np.copyto(host, numpy_array)\n",
    "\n",
    "tic = time.time()    \n",
    "trt_outputs = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream)\n",
    "toc = time.time()\n",
    "dur = toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00020793736991235766\n",
      "0.9427966101694916\n"
     ]
    }
   ],
   "source": [
    "# tensorrt batch infer\n",
    "acc = []\n",
    "trt_time = []\n",
    "confusion_matrix = np.zeros((2, 2) ,int)\n",
    "\n",
    "for data, label in zip(aX_te, ay_te):\n",
    "    inputs[0].host = np.ascontiguousarray(data)\n",
    "    trt_outputs, dur = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream,\n",
    "                        batch_size=1)\n",
    "    acc.append(np.argmax(trt_outputs[1])==label)\n",
    "    confusion_matrix[label, np.argmax(trt_outputs[1])] += 1\n",
    "    trt_time.append(dur)\n",
    "print(np.mean(trt_time))\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[435,   4],\n",
       "       [ 23,  10]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32080,), 2053120)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].host.shape, 80*401*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycuda._driver.DeviceAllocation at 0x7f69e25547b0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15573382377624512\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "trt_outputs = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream)\n",
    "toc = time.time()\n",
    "dur = toc - tic\n",
    "print(dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003384406566619873\n"
     ]
    }
   ],
   "source": [
    "# tensorrt Latency\n",
    "trt_time = []\n",
    "for i in range(1000):\n",
    "    trt_outputs, dur = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream,\n",
    "                        batch_size=1)\n",
    "    trt_time.append(dur)\n",
    "print(np.mean(trt_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021509265899658203\n"
     ]
    }
   ],
   "source": [
    "# tensorrt Latency\n",
    "trt_time = []\n",
    "for i in range(1000):\n",
    "    trt_outputs, dur = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream,\n",
    "                        batch_size=1)\n",
    "    trt_time.append(dur)\n",
    "print(np.mean(trt_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1026560,), 1026560)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].host.shape, 80*401*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Throughput :  4542.682306051055\n",
      "0.00022013425827026366\n"
     ]
    }
   ],
   "source": [
    "# tensorrt Throughput - mmnet, a_model\n",
    "optimal_batch_size = 1\n",
    "repetitions=100\n",
    "total_time = 0\n",
    "trt_time = []\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        _, dur = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream,\n",
    "#                         batch_size=64\n",
    "                        )\n",
    "        curr_time = dur\n",
    "        total_time += curr_time\n",
    "        trt_time.append(dur)\n",
    "Throughput = (repetitions*optimal_batch_size)/total_time\n",
    "print('Final Throughput : ',Throughput)\n",
    "print(np.mean(trt_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
