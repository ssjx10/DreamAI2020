{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from covidxdataset import COVIDxDataset, COVIDxDataset2\n",
    "from audiodataset import CoswaraDataset, ConcatDataset, CoswaraDataset2, CoswaraDataset3\n",
    "import util as util\n",
    "from util import Mel2Samp\n",
    "from train import train, validation, mm_train\n",
    "from model import transfer_resNet, ResNet54, ResNet22, ResNet38, resnet50, MMNet\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "a_model = transfer_resNet(2)\n",
    "# a_model = ResNet22(2)\n",
    "\n",
    "# image\n",
    "i_model = resnet50()\n",
    "num_ftrs = i_model.fc.in_features\n",
    "i_model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "i_model.fc = nn.Linear(num_ftrs, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_model.load_state_dict(torch.load('model/save3/a_model_0.830_0.955'))\n",
    "i_model.load_state_dict(torch.load('model/save3/i_model_0.830_0.955'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmnet = MMNet(a_model, i_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mmnet.state_dict(), './model/mmnet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2245 172\n"
     ]
    }
   ],
   "source": [
    "# dataset_path='../data/Coswara-Data/20*/*/'\n",
    "# pid_list = glob.glob(dataset_path)\n",
    "        \n",
    "# audios = []\n",
    "# paths = []\n",
    "# labels = []\n",
    "# for pid in pid_list:\n",
    "#     json_file = pid + 'metadata.json'\n",
    "#     with open(json_file) as json_file:\n",
    "#         json_data = json.load(json_file)\n",
    "#         status = json_data[\"covid_status\"]\n",
    "#     if status == 'positive_mild' or status == 'positive_moderate':\n",
    "#         status = 'positive'\n",
    "#     if status != 'healthy' and status != 'positive':\n",
    "#         continue\n",
    "#     file_list = glob.glob(pid + '*.wav')\n",
    "#     for f in file_list:\n",
    "#         if 'cough' not in f: #and 'breathing' not in f:\n",
    "#             continue\n",
    "#         paths.append(f)\n",
    "#         labels.append(status)\n",
    "# paths = np.array(paths)\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# n_sample = np.sum(labels == 'positive')\n",
    "# print(np.sum(labels == 'healthy'), n_sample)\n",
    "# # h_paths = paths[labels == 'healthy']\n",
    "# # h_labels = labels[labels == 'healthy']\n",
    "# # idx_sample = np.random.choice(len(h_paths), n_sample)\n",
    "# # new_paths = np.concatenate([h_paths[idx_sample], paths[labels == 'positive']])\n",
    "# # new_labels = np.concatenate([h_labels[idx_sample], labels[labels == 'positive']])\n",
    "\n",
    "# audios = []\n",
    "# for f in paths:\n",
    "#     audio, sr = librosa.load(f, sr=16000)\n",
    "#       if audio.size < 16000*1:\n",
    "#         continue\n",
    "#     audios.append(audio)\n",
    "\n",
    "# np.savez('audios_full', x=audios, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = nn.Sequential(\n",
    "                            AT.MelSpectrogram(sample_rate=16000, \n",
    "                                              n_fft=512, \n",
    "                                              win_length=400,\n",
    "                                              hop_length=160,\n",
    "                                              n_mels=80,\n",
    "                                              f_max=8000\n",
    "                                              ),\n",
    "                            AT.AmplitudeToDB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 401])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrogram(torch.from_numpy(aX[0][:16000*4])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('audios_full.npz', allow_pickle=True)\n",
    "X, y = data['x'], data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2358,), (2358,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg1(audio, segment_length):\n",
    "    max_audio_start = audio.size(0) - segment_length\n",
    "    audio_start = random.randint(0, max_audio_start)\n",
    "    audio_s = audio[audio_start:audio_start + segment_length]\n",
    "    return audio_s\n",
    "    \n",
    "def seg2(audio, segment_length):\n",
    "    max_audio_start = audio.size(0) - segment_length\n",
    "    audio_start = np.argmax(audio) - segment_length//2\n",
    "    if audio_start < 0:\n",
    "        audio_start = 0\n",
    "    if audio_start > max_audio_start:\n",
    "        audio_start = max_audio_start\n",
    "    audio_s = audio[audio_start:audio_start + segment_length]\n",
    "    return audio_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('audios_full.npz', allow_pickle=True)\n",
    "audiox, audioy = data['x'], data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.02,0.07,0.11,0.12,0.01,0.01,0.0,0.00], # 0\n",
    "            [0.09,0.21,0.38,0.35,0.13,0.32,0.33,0.13], # 1\n",
    "            [0.14,0.32,0.56,0.45,0.26,0.55,0.63,0.29], # 2\n",
    "            [0.15,0.53,0.68,0.42,0.37,0.58,0.72,0.34], # 3\n",
    "            [0.15,0.56,0.72,0.53,0.35,0.63,0.74,0.36], # 4\n",
    "            [0.21,0.54,0.74,0.52,0.40,0.71,0.72,0.32], # 5\n",
    "            [0.18,0.41,0.52,0.41,0.32,0.53,0.56,0.22], # 6\n",
    "            [0.04,0.18,0.22,0.23,0.22,0.25,0.22,0.14]] # 7\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor(a)\n",
    "b = b.unsqueeze(0).unsqueeze(0)\n",
    "import torch.nn.functional as F\n",
    "gcam = F.interpolate(\n",
    "            b, (224,224), mode=\"bicubic\", align_corners=False\n",
    "        )\n",
    "b2 = gcam.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length=16000*4\n",
    "len_min = 64000\n",
    "len_max = 0\n",
    "cnt0 = 0\n",
    "mels = []\n",
    "for audio in X:\n",
    "    audio = torch.from_numpy(audio)\n",
    "    # Take segment\n",
    "    if audio.size(0) >= segment_length:\n",
    "        audio = seg2(audio, segment_length)\n",
    "    else:\n",
    "        audio = torch.nn.functional.pad(audio, (0, segment_length - audio.size(0)), 'constant').data\n",
    "    audio = audio.unsqueeze(0)\n",
    "    mel = mel_spectrogram(audio)\n",
    "    mels.append(mel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from util import read_filepaths\n",
    "def load_image(img_path, dim):\n",
    "    if not os.path.exists(img_path):\n",
    "        print(\"IMAGE DOES NOT EXIST {}\".format(img_path))\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = image.resize(dim)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = '/data/CovidX_dataset/test_split.txt'\n",
    "trainfile = '/data/CovidX_dataset/train_split.txt'\n",
    "dataset_path='/data/CovidX_dataset/'\n",
    "\n",
    "paths, labels = read_filepaths(testfile)\n",
    "imgs = []\n",
    "for path in paths:\n",
    "    image = load_image(dataset_path + 'test/' + path, (224,224))\n",
    "    imgs.append(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = aX[0]\n",
    "wn = np.random.randn(len(audio))\n",
    "audio_wn = audio + 0.005*wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('audios_full.npz', allow_pickle=True)\n",
    "aX, ay = data['x'], data['y']\n",
    "data = np.load('images_tr_full.npz', allow_pickle=True)\n",
    "iX_tr, y_tr = data['x'], data['y']\n",
    "data = np.load('images_test.npz', allow_pickle=True)\n",
    "iX_te, y_te = data['x'], data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test examples =  472 [439  33]\n",
      "test examples =  1579 [1479  100]\n",
      "train audio examples =  1886 [1753  133]\n",
      "train image examples =  13957 [13440   517]\n",
      "Class P :  23629081  N :  2693821\n",
      "ImageEpoch: 1\tSample:    1/34380\tLoss:1689.0861\tAccuracy:0.35\n",
      "AudioEpoch: 1\tSample:    1/34380\tLoss:1689.0861\tAccuracy:0.50\n",
      "Training Image\n",
      " SUMMARY EPOCH: 1\tSample:34380/34380\tLoss:175.5489\tAccuracy:0.94\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 1\tSample:34380/34380\tLoss:175.5489\tAccuracy:0.50\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[   97. 17093.]\n",
      " [   95. 17095.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 1\tSample: 1579/ 1579\tLoss:0.0788\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1448.   31.]\n",
      " [  13.   87.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 1\tSample:  472/  472\tLoss:1.9774\tAccuracy:0.07\n",
      "\n",
      "Confusion Matrix\n",
      "[[  0. 439.]\n",
      " [  0.  33.]]\n",
      "ImageEpoch: 2\tSample:    1/34380\tLoss:145.9898\tAccuracy:0.97\n",
      "AudioEpoch: 2\tSample:    1/34380\tLoss:145.9898\tAccuracy:0.46\n",
      "Training Image\n",
      " SUMMARY EPOCH: 2\tSample:34380/34380\tLoss:130.2781\tAccuracy:0.98\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 2\tSample:34380/34380\tLoss:130.2781\tAccuracy:0.58\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[ 2879. 14311.]\n",
      " [   28. 17162.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 2\tSample: 1579/ 1579\tLoss:0.3409\tAccuracy:0.89\n",
      "\n",
      "Confusion Matrix\n",
      "[[1311.  168.]\n",
      " [   2.   98.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 2\tSample:  472/  472\tLoss:0.9164\tAccuracy:0.52\n",
      "\n",
      "Confusion Matrix\n",
      "[[217. 222.]\n",
      " [  4.  29.]]\n",
      "ImageEpoch: 3\tSample:    1/34380\tLoss:111.9804\tAccuracy:0.99\n",
      "AudioEpoch: 3\tSample:    1/34380\tLoss:111.9804\tAccuracy:0.77\n",
      "Training Image\n",
      " SUMMARY EPOCH: 3\tSample:34380/34380\tLoss:101.6610\tAccuracy:0.99\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 3\tSample:34380/34380\tLoss:101.6610\tAccuracy:0.90\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[13955.  3235.]\n",
      " [   60. 17130.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 3\tSample: 1579/ 1579\tLoss:0.1202\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1423.   56.]\n",
      " [   6.   94.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 3\tSample:  472/  472\tLoss:0.3910\tAccuracy:0.85\n",
      "\n",
      "Confusion Matrix\n",
      "[[384.  55.]\n",
      " [ 14.  19.]]\n",
      "ImageEpoch: 4\tSample:    1/34380\tLoss:80.7249\tAccuracy:1.00\n",
      "AudioEpoch: 4\tSample:    1/34380\tLoss:80.7249\tAccuracy:0.98\n",
      "Training Image\n",
      " SUMMARY EPOCH: 4\tSample:34380/34380\tLoss:84.7054\tAccuracy:0.99\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 4\tSample:34380/34380\tLoss:84.7054\tAccuracy:0.98\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[16402.   788.]\n",
      " [   23. 17167.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 4\tSample: 1579/ 1579\tLoss:0.1919\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[1370.  109.]\n",
      " [   5.   95.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 4\tSample:  472/  472\tLoss:0.3257\tAccuracy:0.88\n",
      "\n",
      "Confusion Matrix\n",
      "[[393.  46.]\n",
      " [ 11.  22.]]\n",
      "save!!\n",
      "ImageEpoch: 5\tSample:    1/34380\tLoss:78.6462\tAccuracy:1.00\n",
      "AudioEpoch: 5\tSample:    1/34380\tLoss:78.6462\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH: 5\tSample:34380/34380\tLoss:81.1978\tAccuracy:0.99\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 5\tSample:34380/34380\tLoss:81.1978\tAccuracy:0.99\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.6888e+04 3.0200e+02]\n",
      " [1.1000e+01 1.7179e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 5\tSample: 1579/ 1579\tLoss:0.1154\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1421.   58.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 5\tSample:  472/  472\tLoss:0.2578\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[425.  14.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch: 6\tSample:    1/34380\tLoss:75.5879\tAccuracy:1.00\n",
      "AudioEpoch: 6\tSample:    1/34380\tLoss:75.5879\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH: 6\tSample:34380/34380\tLoss:76.5431\tAccuracy:0.99\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 6\tSample:34380/34380\tLoss:76.5431\tAccuracy:0.99\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7013e+04 1.7700e+02]\n",
      " [7.0000e+00 1.7183e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 6\tSample: 1579/ 1579\tLoss:0.1613\tAccuracy:0.94\n",
      "\n",
      "Confusion Matrix\n",
      "[[1390.   89.]\n",
      " [   4.   96.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 6\tSample:  472/  472\tLoss:0.2929\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[423.  16.]\n",
      " [ 19.  14.]]\n",
      "save!!\n",
      "ImageEpoch: 7\tSample:    1/34380\tLoss:93.7997\tAccuracy:1.00\n",
      "AudioEpoch: 7\tSample:    1/34380\tLoss:93.7997\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH: 7\tSample:34380/34380\tLoss:71.7836\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 7\tSample:34380/34380\tLoss:71.7836\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7106e+04 8.4000e+01]\n",
      " [1.0000e+01 1.7180e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 7\tSample: 1579/ 1579\tLoss:0.2290\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[1366.  113.]\n",
      " [   5.   95.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 7\tSample:  472/  472\tLoss:0.3545\tAccuracy:0.90\n",
      "\n",
      "Confusion Matrix\n",
      "[[412.  27.]\n",
      " [ 19.  14.]]\n",
      "save!!\n",
      "ImageEpoch: 8\tSample:    1/34380\tLoss:67.8598\tAccuracy:1.00\n",
      "AudioEpoch: 8\tSample:    1/34380\tLoss:67.8598\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH: 8\tSample:34380/34380\tLoss:71.2978\tAccuracy:0.99\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 8\tSample:34380/34380\tLoss:71.2978\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7145e+04 4.5000e+01]\n",
      " [2.0000e+00 1.7188e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 8\tSample: 1579/ 1579\tLoss:0.1366\tAccuracy:0.95\n",
      "\n",
      "Confusion Matrix\n",
      "[[1406.   73.]\n",
      " [   5.   95.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 8\tSample:  472/  472\tLoss:0.2955\tAccuracy:0.94\n",
      "\n",
      "Confusion Matrix\n",
      "[[433.   6.]\n",
      " [ 23.  10.]]\n",
      "save!!\n",
      "Epoch     8: reducing learning rate of group 0 to 2.5000e-05.\n",
      "ImageEpoch: 9\tSample:    1/34380\tLoss:61.1308\tAccuracy:1.00\n",
      "AudioEpoch: 9\tSample:    1/34380\tLoss:61.1308\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH: 9\tSample:34380/34380\tLoss:69.5865\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH: 9\tSample:34380/34380\tLoss:69.5865\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7156e+04 3.4000e+01]\n",
      " [4.0000e+00 1.7186e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH: 9\tSample: 1579/ 1579\tLoss:0.1069\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1425.   54.]\n",
      " [   9.   91.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH: 9\tSample:  472/  472\tLoss:0.2945\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[429.  10.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch:10\tSample:    1/34380\tLoss:60.7523\tAccuracy:1.00\n",
      "AudioEpoch:10\tSample:    1/34380\tLoss:60.7523\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:10\tSample:34380/34380\tLoss:62.6761\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:10\tSample:34380/34380\tLoss:62.6761\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7165e+04 2.5000e+01]\n",
      " [1.0000e+00 1.7189e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:10\tSample: 1579/ 1579\tLoss:0.0935\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1437.   42.]\n",
      " [  11.   89.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:10\tSample:  472/  472\tLoss:0.3315\tAccuracy:0.92\n",
      "\n",
      "Confusion Matrix\n",
      "[[418.  21.]\n",
      " [ 18.  15.]]\n",
      "save!!\n",
      "ImageEpoch:11\tSample:    1/34380\tLoss:62.9186\tAccuracy:1.00\n",
      "AudioEpoch:11\tSample:    1/34380\tLoss:62.9186\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:11\tSample:34380/34380\tLoss:60.2891\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:11\tSample:34380/34380\tLoss:60.2891\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[17170.    20.]\n",
      " [    0. 17190.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:11\tSample: 1579/ 1579\tLoss:0.0969\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1429.   50.]\n",
      " [  11.   89.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:11\tSample:  472/  472\tLoss:0.3222\tAccuracy:0.92\n",
      "\n",
      "Confusion Matrix\n",
      "[[423.  16.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "Epoch    11: reducing learning rate of group 0 to 1.2500e-05.\n",
      "ImageEpoch:12\tSample:    1/34380\tLoss:61.3819\tAccuracy:0.99\n",
      "AudioEpoch:12\tSample:    1/34380\tLoss:61.3819\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:12\tSample:34380/34380\tLoss:56.7041\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:12\tSample:34380/34380\tLoss:56.7041\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.718e+04 1.000e+01]\n",
      " [0.000e+00 1.719e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:12\tSample: 1579/ 1579\tLoss:0.1447\tAccuracy:0.95\n",
      "\n",
      "Confusion Matrix\n",
      "[[1406.   73.]\n",
      " [   7.   93.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:12\tSample:  472/  472\tLoss:0.3072\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 20.  13.]]\n",
      "save!!\n",
      "ImageEpoch:13\tSample:    1/34380\tLoss:57.1092\tAccuracy:1.00\n",
      "AudioEpoch:13\tSample:    1/34380\tLoss:57.1092\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:13\tSample:34380/34380\tLoss:56.7066\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:13\tSample:34380/34380\tLoss:56.7066\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7179e+04 1.1000e+01]\n",
      " [1.0000e+00 1.7189e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:13\tSample: 1579/ 1579\tLoss:0.1105\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1425.   54.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:13\tSample:  472/  472\tLoss:0.3529\tAccuracy:0.92\n",
      "\n",
      "Confusion Matrix\n",
      "[[418.  21.]\n",
      " [ 16.  17.]]\n",
      "save!!\n",
      "ImageEpoch:14\tSample:    1/34380\tLoss:70.6077\tAccuracy:1.00\n",
      "AudioEpoch:14\tSample:    1/34380\tLoss:70.6077\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:14\tSample:34380/34380\tLoss:55.4916\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:14\tSample:34380/34380\tLoss:55.4916\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7182e+04 8.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:14\tSample: 1579/ 1579\tLoss:0.1329\tAccuracy:0.95\n",
      "\n",
      "Confusion Matrix\n",
      "[[1410.   69.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:14\tSample:  472/  472\tLoss:0.3104\tAccuracy:0.94\n",
      "\n",
      "Confusion Matrix\n",
      "[[428.  11.]\n",
      " [ 19.  14.]]\n",
      "save!!\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-06.\n",
      "ImageEpoch:15\tSample:    1/34380\tLoss:51.1609\tAccuracy:1.00\n",
      "AudioEpoch:15\tSample:    1/34380\tLoss:51.1609\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:15\tSample:34380/34380\tLoss:54.4816\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:15\tSample:34380/34380\tLoss:54.4816\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7184e+04 6.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:15\tSample: 1579/ 1579\tLoss:0.1191\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1418.   61.]\n",
      " [   7.   93.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:15\tSample:  472/  472\tLoss:0.3256\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 20.  13.]]\n",
      "save!!\n",
      "ImageEpoch:16\tSample:    1/34380\tLoss:56.1867\tAccuracy:1.00\n",
      "AudioEpoch:16\tSample:    1/34380\tLoss:56.1867\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:16\tSample:34380/34380\tLoss:53.5876\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:16\tSample:34380/34380\tLoss:53.5876\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7182e+04 8.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:16\tSample: 1579/ 1579\tLoss:0.1487\tAccuracy:0.95\n",
      "\n",
      "Confusion Matrix\n",
      "[[1406.   73.]\n",
      " [   7.   93.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:16\tSample:  472/  472\tLoss:0.3306\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 20.  13.]]\n",
      "save!!\n",
      "ImageEpoch:17\tSample:    1/34380\tLoss:62.6294\tAccuracy:1.00\n",
      "AudioEpoch:17\tSample:    1/34380\tLoss:62.6294\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:17\tSample:34380/34380\tLoss:54.0765\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:17\tSample:34380/34380\tLoss:54.0765\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7184e+04 6.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:17\tSample: 1579/ 1579\tLoss:0.0951\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1436.   43.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:17\tSample:  472/  472\tLoss:0.3354\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 19.  14.]]\n",
      "save!!\n",
      "Epoch    17: reducing learning rate of group 0 to 3.1250e-06.\n",
      "ImageEpoch:18\tSample:    1/34380\tLoss:61.6607\tAccuracy:1.00\n",
      "AudioEpoch:18\tSample:    1/34380\tLoss:61.6607\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:18\tSample:34380/34380\tLoss:53.4197\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:18\tSample:34380/34380\tLoss:53.4197\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.718e+04 1.000e+01]\n",
      " [0.000e+00 1.719e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:18\tSample: 1579/ 1579\tLoss:0.0963\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1436.   43.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:18\tSample:  472/  472\tLoss:0.3273\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[423.  16.]\n",
      " [ 18.  15.]]\n",
      "save!!\n",
      "ImageEpoch:19\tSample:    1/34380\tLoss:52.7800\tAccuracy:1.00\n",
      "AudioEpoch:19\tSample:    1/34380\tLoss:52.7800\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:19\tSample:34380/34380\tLoss:52.8263\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:19\tSample:34380/34380\tLoss:52.8263\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7182e+04 8.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:19\tSample: 1579/ 1579\tLoss:0.0951\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1436.   43.]\n",
      " [   9.   91.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:19\tSample:  472/  472\tLoss:0.3329\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 18.  15.]]\n",
      "save!!\n",
      "ImageEpoch:20\tSample:    1/34380\tLoss:43.9658\tAccuracy:1.00\n",
      "AudioEpoch:20\tSample:    1/34380\tLoss:43.9658\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:20\tSample:34380/34380\tLoss:52.4995\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:20\tSample:34380/34380\tLoss:52.4995\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.718e+04 1.000e+01]\n",
      " [0.000e+00 1.719e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:20\tSample: 1579/ 1579\tLoss:0.0966\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1435.   44.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:20\tSample:  472/  472\tLoss:0.3292\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[425.  14.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "Epoch    20: reducing learning rate of group 0 to 1.5625e-06.\n",
      "ImageEpoch:21\tSample:    1/34380\tLoss:41.0531\tAccuracy:1.00\n",
      "AudioEpoch:21\tSample:    1/34380\tLoss:41.0531\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:21\tSample:34380/34380\tLoss:51.7074\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:21\tSample:34380/34380\tLoss:51.7074\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7181e+04 9.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:21\tSample: 1579/ 1579\tLoss:0.0804\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1443.   36.]\n",
      " [  10.   90.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:21\tSample:  472/  472\tLoss:0.3459\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[427.  12.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch:22\tSample:    1/34380\tLoss:56.3665\tAccuracy:1.00\n",
      "AudioEpoch:22\tSample:    1/34380\tLoss:56.3665\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:22\tSample:34380/34380\tLoss:51.9305\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:22\tSample:34380/34380\tLoss:51.9305\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7182e+04 8.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:22\tSample: 1579/ 1579\tLoss:0.0842\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1441.   38.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:22\tSample:  472/  472\tLoss:0.3424\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[428.  11.]\n",
      " [ 22.  11.]]\n",
      "save!!\n",
      "ImageEpoch:23\tSample:    1/34380\tLoss:53.8788\tAccuracy:1.00\n",
      "AudioEpoch:23\tSample:    1/34380\tLoss:53.8788\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:23\tSample:34380/34380\tLoss:51.1546\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:23\tSample:34380/34380\tLoss:51.1546\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7185e+04 5.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:23\tSample: 1579/ 1579\tLoss:0.0991\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1436.   43.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:23\tSample:  472/  472\tLoss:0.3374\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 19.  14.]]\n",
      "save!!\n",
      "Epoch    23: reducing learning rate of group 0 to 7.8125e-07.\n",
      "ImageEpoch:24\tSample:    1/34380\tLoss:69.5702\tAccuracy:1.00\n",
      "AudioEpoch:24\tSample:    1/34380\tLoss:69.5702\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:24\tSample:34380/34380\tLoss:51.9150\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:24\tSample:34380/34380\tLoss:51.9150\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7187e+04 3.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:24\tSample: 1579/ 1579\tLoss:0.0764\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1449.   30.]\n",
      " [  10.   90.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:24\tSample:  472/  472\tLoss:0.3345\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 20.  13.]]\n",
      "save!!\n",
      "ImageEpoch:25\tSample:    1/34380\tLoss:50.4023\tAccuracy:1.00\n",
      "AudioEpoch:25\tSample:    1/34380\tLoss:50.4023\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:25\tSample:34380/34380\tLoss:51.6881\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:25\tSample:34380/34380\tLoss:51.6881\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7188e+04 2.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:25\tSample: 1579/ 1579\tLoss:0.0948\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1433.   46.]\n",
      " [   9.   91.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:25\tSample:  472/  472\tLoss:0.3458\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[429.  10.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch:26\tSample:    1/34380\tLoss:60.7310\tAccuracy:1.00\n",
      "AudioEpoch:26\tSample:    1/34380\tLoss:60.7310\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:26\tSample:34380/34380\tLoss:51.2569\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:26\tSample:34380/34380\tLoss:51.2569\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7188e+04 2.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:26\tSample: 1579/ 1579\tLoss:0.0829\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1443.   36.]\n",
      " [  10.   90.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:26\tSample:  472/  472\tLoss:0.3369\tAccuracy:0.94\n",
      "\n",
      "Confusion Matrix\n",
      "[[432.   7.]\n",
      " [ 22.  11.]]\n",
      "save!!\n",
      "Epoch    26: reducing learning rate of group 0 to 3.9063e-07.\n",
      "ImageEpoch:27\tSample:    1/34380\tLoss:60.7443\tAccuracy:1.00\n",
      "AudioEpoch:27\tSample:    1/34380\tLoss:60.7443\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:27\tSample:34380/34380\tLoss:51.8591\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:27\tSample:34380/34380\tLoss:51.8591\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7185e+04 5.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:27\tSample: 1579/ 1579\tLoss:0.0874\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1438.   41.]\n",
      " [  10.   90.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:27\tSample:  472/  472\tLoss:0.3318\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[428.  11.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch:28\tSample:    1/34380\tLoss:45.9629\tAccuracy:1.00\n",
      "AudioEpoch:28\tSample:    1/34380\tLoss:45.9629\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:28\tSample:34380/34380\tLoss:52.0355\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:28\tSample:34380/34380\tLoss:52.0355\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7186e+04 4.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:28\tSample: 1579/ 1579\tLoss:0.0638\tAccuracy:0.98\n",
      "\n",
      "Confusion Matrix\n",
      "[[1455.   24.]\n",
      " [  13.   87.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:28\tSample:  472/  472\tLoss:0.3315\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[428.  11.]\n",
      " [ 22.  11.]]\n",
      "save!!\n",
      "ImageEpoch:29\tSample:    1/34380\tLoss:48.0841\tAccuracy:1.00\n",
      "AudioEpoch:29\tSample:    1/34380\tLoss:48.0841\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:29\tSample:34380/34380\tLoss:51.0999\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:29\tSample:34380/34380\tLoss:51.0999\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7187e+04 3.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:29\tSample: 1579/ 1579\tLoss:0.0895\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1437.   42.]\n",
      " [   9.   91.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:29\tSample:  472/  472\tLoss:0.3323\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[427.  12.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "Epoch    29: reducing learning rate of group 0 to 1.9531e-07.\n",
      "ImageEpoch:30\tSample:    1/34380\tLoss:43.3325\tAccuracy:1.00\n",
      "AudioEpoch:30\tSample:    1/34380\tLoss:43.3325\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:30\tSample:34380/34380\tLoss:51.9380\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:30\tSample:34380/34380\tLoss:51.9380\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7186e+04 4.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:30\tSample: 1579/ 1579\tLoss:0.0836\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1439.   40.]\n",
      " [  10.   90.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:30\tSample:  472/  472\tLoss:0.3332\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[429.  10.]\n",
      " [ 22.  11.]]\n",
      "save!!\n",
      "ImageEpoch:31\tSample:    1/34380\tLoss:47.3228\tAccuracy:1.00\n",
      "AudioEpoch:31\tSample:    1/34380\tLoss:47.3228\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:31\tSample:34380/34380\tLoss:51.2846\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:31\tSample:34380/34380\tLoss:51.2846\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[17190.     0.]\n",
      " [    0. 17190.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:31\tSample: 1579/ 1579\tLoss:0.0961\tAccuracy:0.97\n",
      "\n",
      "Confusion Matrix\n",
      "[[1435.   44.]\n",
      " [   8.   92.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:31\tSample:  472/  472\tLoss:0.3367\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[426.  13.]\n",
      " [ 20.  13.]]\n",
      "save!!\n",
      "ImageEpoch:32\tSample:    1/34380\tLoss:57.0068\tAccuracy:1.00\n",
      "AudioEpoch:32\tSample:    1/34380\tLoss:57.0068\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:32\tSample:34380/34380\tLoss:51.4396\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:32\tSample:34380/34380\tLoss:51.4396\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[17190.     0.]\n",
      " [    0. 17190.]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:32\tSample: 1579/ 1579\tLoss:0.0726\tAccuracy:0.98\n",
      "\n",
      "Confusion Matrix\n",
      "[[1453.   26.]\n",
      " [  11.   89.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:32\tSample:  472/  472\tLoss:0.3356\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[427.  12.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-07.\n",
      "ImageEpoch:33\tSample:    1/34380\tLoss:49.9770\tAccuracy:1.00\n",
      "AudioEpoch:33\tSample:    1/34380\tLoss:49.9770\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:33\tSample:34380/34380\tLoss:51.5370\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:33\tSample:34380/34380\tLoss:51.5370\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7185e+04 5.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:33\tSample: 1579/ 1579\tLoss:0.0664\tAccuracy:0.98\n",
      "\n",
      "Confusion Matrix\n",
      "[[1457.   22.]\n",
      " [  14.   86.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:33\tSample:  472/  472\tLoss:0.3310\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[428.  11.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch:34\tSample:    1/34380\tLoss:48.7657\tAccuracy:1.00\n",
      "AudioEpoch:34\tSample:    1/34380\tLoss:48.7657\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:34\tSample:34380/34380\tLoss:51.0153\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:34\tSample:34380/34380\tLoss:51.0153\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7188e+04 2.0000e+00]\n",
      " [0.0000e+00 1.7190e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:34\tSample: 1579/ 1579\tLoss:0.0722\tAccuracy:0.98\n",
      "\n",
      "Confusion Matrix\n",
      "[[1452.   27.]\n",
      " [  12.   88.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:34\tSample:  472/  472\tLoss:0.3273\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[429.  10.]\n",
      " [ 21.  12.]]\n",
      "save!!\n",
      "ImageEpoch:35\tSample:    1/34380\tLoss:56.8649\tAccuracy:1.00\n",
      "AudioEpoch:35\tSample:    1/34380\tLoss:56.8649\tAccuracy:1.00\n",
      "Training Image\n",
      " SUMMARY EPOCH:35\tSample:34380/34380\tLoss:52.1420\tAccuracy:1.00\n",
      "\n",
      "Training Audio\n",
      " SUMMARY EPOCH:35\tSample:34380/34380\tLoss:52.1420\tAccuracy:1.00\n",
      "\n",
      "A_Confusion Matrix\n",
      "[[1.7184e+04 6.0000e+00]\n",
      " [1.0000e+00 1.7189e+04]]\n",
      "\n",
      "Image: Validation\n",
      " SUMMARY EPOCH:35\tSample: 1579/ 1579\tLoss:0.1313\tAccuracy:0.96\n",
      "\n",
      "Confusion Matrix\n",
      "[[1416.   63.]\n",
      " [   7.   93.]]\n",
      "Audio: Validation\n",
      " SUMMARY EPOCH:35\tSample:  472/  472\tLoss:0.3356\tAccuracy:0.93\n",
      "\n",
      "Confusion Matrix\n",
      "[[427.  12.]\n",
      " [ 21.  12.]]\n",
      "save!!\n"
     ]
    }
   ],
   "source": [
    "seed = 20\n",
    "seed_everything(seed)\n",
    "aX_tr, aX_te, ay_tr, ay_te = train_test_split(aX, ay, test_size=0.2, shuffle=True, stratify=ay, random_state=seed)\n",
    "\n",
    "num_classes = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# audio\n",
    "a_model = transfer_resNet(num_classes)\n",
    "# a_model = ResNet22(2)\n",
    "a_model.to(device)\n",
    "# image\n",
    "i_model = resnet50(pretrained=True)\n",
    "num_ftrs = i_model.fc.in_features\n",
    "i_model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "i_model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "i_model.to(device)\n",
    "\n",
    "batch_size = 96\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 15}\n",
    "\n",
    "test_params = {'batch_size': batch_size,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 15}\n",
    "\n",
    "optimizer = torch.optim.Adam(list(i_model.parameters()) + list(a_model.parameters()), lr=0.00005)\n",
    "\n",
    "# cos_train_dataset = CoswaraDataset(mode='train', n_classes=num_classes, segment_length=16000*4)\n",
    "# cos_val_dataset = CoswaraDataset(mode='valid', n_classes=num_classes, segment_length=16000*4)\n",
    "# cos_test_dataset = CoswaraDataset(mode='test', n_classes=num_classes, segment_length=16000*4)\n",
    "\n",
    "# cos_train_dataset = CoswaraDataset2(x,y, mode='train', segment_length=16000*4)\n",
    "# cos_test_dataset = CoswaraDataset2(x_test,y_test, mode='test', segment_length=16000*4)\n",
    "\n",
    "# cos_train_dataset = CoswaraDataset3(aX_tr, ay_tr, mode='train')\n",
    "cos_test_dataset = CoswaraDataset2(aX_te, ay_te, mode='test')\n",
    "                                    \n",
    "# cx_train_dataset = COVIDxDataset(mode='train', n_classes=num_classes, dim=(224, 224))\n",
    "# cx_val_dataset = COVIDxDataset(mode='valid', n_classes=num_classes, dim=(224, 224))\n",
    "# cx_test_dataset = COVIDxDataset(mode='test', n_classes=num_classes, dim=(224, 224))\n",
    "\n",
    "# cx_train_dataset = COVIDxDataset2(iX_tr, y_tr, mode='train')\n",
    "cx_test_dataset = COVIDxDataset2(iX_te, y_te, mode='test')\n",
    "    \n",
    "train_dataset = ConcatDataset(aX_tr, ay_tr, iX_tr, y_tr, mode='train')\n",
    "# val_dataset = ConcatDataset(cos_val_dataset, cx_val_dataset)\n",
    "# test_dataset = ConcatDataset(cos_test_dataset, cx_test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_params)\n",
    "# val_loader = DataLoader(val_dataset, **test_params)\n",
    "###\n",
    "i_test_loader = DataLoader(cx_test_dataset, **test_params)\n",
    "o_test_loader = DataLoader(cos_test_dataset, **test_params)\n",
    "\n",
    "# print(model)\n",
    "num_epochs = 35\n",
    "best_pred_loss = 1000.0\n",
    "lr_sch = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, min_lr=1e-7, verbose=True)\n",
    "# lr_sch = ExponentialLR(optimizer, gamma=0.975)\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    mm_train(device, batch_size, a_model, i_model, train_loader, optimizer, epoch, None)\n",
    "    print('Image:', end=' ')\n",
    "    i_val_metrics, confusion_matrix = validation(device, batch_size, num_classes, i_model, i_test_loader, epoch, None)\n",
    "    print('Audio:', end=' ')\n",
    "    val_metrics, confusion_matrix = validation(device, batch_size, num_classes, a_model, o_test_loader, epoch, None)\n",
    "    \n",
    "    \n",
    "    random.shuffle(train_dataset.Training_nn)\n",
    "    random.shuffle(train_dataset.Training_np)\n",
    "    random.shuffle(train_dataset.Training_pn)\n",
    "    random.shuffle(train_dataset.Training_pp)\n",
    "    len_pp = len(train_dataset.Training_pp)\n",
    "    train_dataset.datas = train_dataset.Training_nn[:len_pp//8] + train_dataset.Training_pp[:len_pp//8] + train_dataset.Training_np[:len_pp//8] + train_dataset.Training_pn[:len_pp//8]\n",
    "    \n",
    "    if val_metrics.avg('accuracy') > 0.87:\n",
    "        best_acc = val_metrics.avg('accuracy')\n",
    "        torch.save(a_model.state_dict(), '/model/save1/f2_a_model_' + str(best_acc)[:5] + '_' +str(val_metrics.avg('accuracy'))[:5])\n",
    "        torch.save(i_model.state_dict(), '/model/save1/f2_i_model_' + str(best_acc)[:5] + '_' +str(i_val_metrics.avg('accuracy'))[:5])\n",
    "        print('save!!')\n",
    "                   \n",
    "    lr_sch.step(val_metrics.avg('loss'))\n",
    "#     lr_sch.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random, shift\n",
    "Audio: Validation\n",
    " SUMMARY EPOCH:20\tSample:  472/  472\tLoss:0.3834\tAccuracy:0.87\n",
    "\n",
    "Confusion Matrix\n",
    "[[390.  49.]\n",
    " [ 11.  22.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio: Validation\n",
    " SUMMARY EPOCH:16\tSample:  472/  472\tLoss:0.4944\tAccuracy:0.84\n",
    "\n",
    "Confusion Matrix\n",
    "[[376.  63.]\n",
    " [ 11.  22.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_ce + (1 - alpha) * a_ce + alpha * csa 0.5 / 0.00005 / mel\n",
    "# shuffle // 10 -> 6708 (random) @save3 pretrain x\n",
    "train examples =  259 [130 129]\n",
    "test examples =  65 [32 33]\n",
    "Training Image\n",
    " SUMMARY EPOCH:32\tSample: 6708/ 6708\tLoss:32.9175\tAccuracy:1.00\n",
    "\n",
    "Training Audio\n",
    " SUMMARY EPOCH:32\tSample: 6708/ 6708\tLoss:32.9175\tAccuracy:1.00\n",
    "\n",
    "A_Confusion Matrix\n",
    "[[3.374e+03 2.000e+00]\n",
    " [2.200e+01 3.310e+03]]\n",
    "\n",
    "Image: Validation\n",
    " SUMMARY EPOCH:32\tSample: 1579/ 1579\tLoss:0.1712\tAccuracy:0.95\n",
    "\n",
    "Confusion Matrix\n",
    "[[1415.   64.]\n",
    " [  19.   81.]]\n",
    "Audio: Validation\n",
    " SUMMARY EPOCH:32\tSample:   65/   65\tLoss:0.3008\tAccuracy:0.83\n",
    "\n",
    "Confusion Matrix\n",
    "[[28.  4.]\n",
    " [ 7. 26.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_ce + (1 - alpha) * a_ce + alpha * csa 0.5 / 0.00005 / mel\n",
    "# shuffle // 10 -> 6708 (random) @save2 pretrain x\n",
    "Image: Validation\n",
    " SUMMARY EPOCH: 8\tSample: 1579/ 1579\tLoss:0.3632\tAccuracy:0.88\n",
    "\n",
    "Confusion Matrix\n",
    "[[1305.  174.]\n",
    " [  13.   87.]]\n",
    "Audio: Validation\n",
    " SUMMARY EPOCH: 8\tSample:   65/   65\tLoss:0.1928\tAccuracy:0.86\n",
    "\n",
    "Confusion Matrix\n",
    "[[27.  5.]\n",
    " [ 4. 29.]]\n",
    "save!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_ce + (1 - alpha) * a_ce + alpha * csa 0.7 / 0.00005 / mel\n",
    "# shuffle // 10 -> 6708 (random) @save1\n",
    "Image: Validation\n",
    " SUMMARY EPOCH:29\tSample: 1579/ 1579\tLoss:0.3980\tAccuracy:0.87\n",
    "\n",
    "Confusion Matrix\n",
    "[[1288.  191.]\n",
    " [  10.   90.]]\n",
    "Audio: Validation\n",
    " SUMMARY EPOCH:29\tSample:   65/   65\tLoss:0.5410\tAccuracy:0.80\n",
    "\n",
    "Confusion Matrix\n",
    "[[26.  6.]\n",
    " [ 7. 26.]]\n",
    "save!!\n",
    "# i_ce + (1 - alpha) * a_ce + alpha * csa 0.5 / 0.00005 / mel\n",
    "# shuffle // 10 -> 6708 (random)\n",
    "Training Image\n",
    " SUMMARY EPOCH:29\tSample: 6708/ 6708\tLoss:45.2970\tAccuracy:1.00\n",
    "\n",
    "Training Audio\n",
    " SUMMARY EPOCH:29\tSample: 6708/ 6708\tLoss:45.2970\tAccuracy:0.98\n",
    "\n",
    "A_Confusion Matrix\n",
    "[[3322.   64.]\n",
    " [ 102. 3220.]]\n",
    "\n",
    "Image: Validation\n",
    " SUMMARY EPOCH:29\tSample: 1579/ 1579\tLoss:0.2261\tAccuracy:0.93\n",
    "\n",
    "Confusion Matrix\n",
    "[[1388.   91.]\n",
    " [  14.   86.]]\n",
    "Audio: Validation\n",
    " SUMMARY EPOCH:29\tSample:   65/   65\tLoss:0.5479\tAccuracy:0.80\n",
    "\n",
    "Confusion Matrix\n",
    "[[26.  6.]\n",
    " [ 7. 26.]]\n",
    "# i_ce + (1 - alpha) * a_ce + alpha * csa 0.5 / 0.00005 / mel\n",
    "# shuffle // 10 -> 6708\n",
    "Training Image\n",
    " SUMMARY EPOCH:18\tSample: 6708/ 6708\tLoss:39.7886\tAccuracy:1.00\n",
    "\n",
    "Training Audio\n",
    " SUMMARY EPOCH:18\tSample: 6708/ 6708\tLoss:39.7886\tAccuracy:0.99\n",
    "\n",
    "A_Confusion Matrix\n",
    "[[3313.   30.]\n",
    " [  36. 3329.]]\n",
    "\n",
    "Image: Validation\n",
    " SUMMARY EPOCH:18\tSample: 1579/ 1579\tLoss:0.2895\tAccuracy:0.90\n",
    "\n",
    "Confusion Matrix\n",
    "[[1338.  141.]\n",
    " [  11.   89.]]\n",
    "Audio: Validation\n",
    " SUMMARY EPOCH:18\tSample:   65/   65\tLoss:0.7218\tAccuracy:0.80\n",
    "\n",
    "Confusion Matrix\n",
    "[[26.  6.]\n",
    " [ 7. 26.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, confusion_matrix = validation(device, batch_size, num_classes, model, test_loader, epoch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
